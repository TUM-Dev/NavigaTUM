import json
import logging
import urllib.error
import urllib.request
import xml.etree.ElementTree as ET  # nosec: used for writing to a file, not for reading
from datetime import datetime
from typing import Union

from compile import DEBUG_MODE
from defusedxml import ElementTree as defusedET  # type:ignore

# defusedxml only supports parse()


def generate_sitemap():
    """Generate a sitemap that diffs changes since to the currently online data"""

    if DEBUG_MODE:
        logging.info("Skipping sitemap generation in Dev Mode (GIT_COMMIT_SHA is unset)")
        return

    # Load exported data. This function is intentionally not using the data object
    # directly, but re-parsing the output file instead, because the export not
    # export all fields. This way we're also guaranteed to have the same types
    # (and not e.g. numpy floats).
    with open("output/api_data.json", encoding="utf-8") as file:
        new_data = json.load(file)

    # Currently online data
    req = urllib.request.Request("https://nav.tum.sexy/cdn/api_data.json")
    with urllib.request.urlopen(req) as resp:  # nosec: url parameter is fixed and does not allow for file traversal
        old_data = json.loads(resp.read().decode("utf-8"))

    # Look whether there are currently online sitemaps for the provided
    # sitemaps name. In case there aren't, we assume this sitemap is new,
    # and all entries will be marked as changed.
    old_sitemaps = _download_online_sitemaps(["room", "other"])

    sitemaps = _extract_sitemap_data(new_data, old_data, old_sitemaps)

    for name, sitemap in sitemaps.items():
        _write_sitemap_xml(f"output/sitemap-data-{name}.xml", sitemap)

    _write_sitemapindex_xml("output/sitemap.xml", sitemaps)


def _extract_sitemap_data(new_data, old_data, old_sitemaps) -> dict[str, list[dict[str, Union[str, float, datetime]]]]:
    """
    Extract sitemap data.
    Lastmod is set to the current time if the entry is modified (idicated via comparing newdata vs olddata),
    or to the last modification time of the online sitemap if the entry is not modified.
    """

    # Each sitemap has a limit of 50MB uncompressed or 50000 entries
    # (that means 1KB per site). We have currently about 33000 entries,
    # so it's unlikely that we'll hit this limit without adding a lot of
    # data. But for the case that a new type of entry is introduced, the
    # sitemap is split into one for rooms and one for the rest.
    # Note that the root element is not included, because it just redirects
    # to the main page.
    sitemaps: dict[str, list[dict[str, Union[str, float, datetime]]]] = {
        "room": [],
        "other": [],
    }

    changed_count = 0
    for _id, entry in new_data.items():
        if entry["type"] == "root":
            continue

        sitemap_name = entry["type"] if entry["type"] in sitemaps else "other"

        # Just copied from the webclient. The webclient doesn't care about
        # the prefix â€“ if it is wrong it'll be corrected (without a redirect).
        # However this way search engines can already index the final URL.
        url_type_name = {
            "campus": "campus",
            "site": "site",
            "area": "site",
            "building": "building",
            "joined_building": "building",
            "room": "room",
            "virtual_room": "room",
        }[entry["type"]]
        url = f"https://nav.tum.sexy/{url_type_name}/{_id}"
        if _id not in old_data or entry != old_data[_id]:
            lastmod = datetime.utcnow()
            changed_count += 1
        else:
            # Try to look up the last changed date in the old sitemap
            lastmod = old_sitemaps.get(sitemap_name, {}).get(url, None)
            if lastmod is None:
                lastmod = datetime.utcnow()
                changed_count += 1

        # Priority is a relative measure from 0.0 to 1.0.
        # The data's `ranking_factors` have arbitrary scaling, but are for
        # rooms in general in the range 0 to 900, so we just add 100, divide by 10_000
        # and clamp to 1.0 for rooms.
        # For buildings etc. that are always >= 10_000, we just subtract 500
        # to get some kind of relative measure.
        if entry["type"] == "room":
            priority = min((entry["ranking_factors"]["rank_combined"] + 100) / 10000, 1.0)
        else:
            priority = min((entry["ranking_factors"]["rank_combined"] - 500) / 10000, 1.0)

        sitemaps[sitemap_name].append(
            {
                "url": url,
                "lastmod": lastmod,
                "priority": priority,
            },
        )
    logging.info(f"{changed_count} of {len(new_data) - 1} URLs have been updated.")

    return sitemaps


def _download_online_sitemaps(sitemap_names):
    """Download online sitemaps by their names"""
    sitemaps = {}
    for name in sitemap_names:
        sitemaps[name] = _download_online_sitemap(f"https://nav.tum.sexy/cdn/sitemap-data-{name}.xml")
    return sitemaps


def _download_online_sitemap(url):
    xmlns = "{http://www.sitemaps.org/schemas/sitemap/0.9}"  # noqa: FS003
    req = urllib.request.Request(url)
    try:
        with urllib.request.urlopen(req) as resp:  # nosec: url parameter is fixed and does not allow for file traversal
            sitemap_str = resp.read().decode("utf-8")
            sitemap = {}
            root = defusedET.fromstring(sitemap_str)
            for child in root.iter(f"{xmlns}url"):
                loc = child.find(f"{xmlns}loc")
                lastmod = child.find(f"{xmlns}lastmod")
                if loc is not None and lastmod is not None:
                    sitemap[loc.text] = datetime.fromisoformat(lastmod.text.rstrip("Z"))
    except urllib.error.HTTPError as error:
        logging.warning(f"Failed to download sitemap '{url}': {error}")
    return sitemap


def _write_sitemap_xml(fname, sitemap):
    """Write the sitemap XML for a single sitemap"""
    urlset = ET.Element("urlset")
    urlset.set("xmlns", "http://www.sitemaps.org/schemas/sitemap/0.9")
    for sitemap_entry in sitemap:
        url = ET.SubElement(urlset, "url")
        loc = ET.SubElement(url, "loc")
        loc.text = sitemap_entry["url"]
        lastmod = ET.SubElement(url, "lastmod")
        lastmod.text = sitemap_entry["lastmod"].isoformat(timespec="seconds") + "Z"
        priority = ET.SubElement(url, "priority")
        priority.text = str(round(sitemap_entry["priority"], 2))

    root = ET.ElementTree(urlset)
    root.write(fname, encoding="utf-8", xml_declaration=True)


def _write_sitemapindex_xml(fname, sitemaps):
    """Write the sitemapindex XML"""
    sitemapindex = ET.Element("sitemapindex")
    sitemapindex.set("xmlns", "http://www.sitemaps.org/schemas/sitemap/0.9")
    for name, sitemap in sitemaps.items():
        sitemap_el = ET.SubElement(sitemapindex, "sitemap")
        loc = ET.SubElement(sitemap_el, "loc")
        loc.text = f"https://nav.tum.sexy/cdn/sitemap-data-{name}.xml"
        # we set the lastmod to the latest lastmod of all sitemaps
        lastmod_dates = {site["lastmod"] for site in sitemap if "lastmod" in site}
        if lastmod_dates:
            lastmod = ET.SubElement(sitemap_el, "lastmod")
            lastmod.text = max(lastmod_dates).isoformat(timespec="seconds") + "Z"

    # Because sitemaps cannot be hierarchical, we have to include the
    # webclient sitemap here as well.
    sitemap_el = ET.SubElement(sitemapindex, "sitemap")
    loc = ET.SubElement(sitemap_el, "loc")
    web_sitemap_url = "https://nav.tum.sexy/sitemap-webclient.xml"
    loc.text = web_sitemap_url
    sitemap = _download_online_sitemap(web_sitemap_url)
    lastmod_dates = set(sitemap.values())
    if lastmod_dates:
        lastmod = ET.SubElement(sitemap_el, "lastmod")
        lastmod.text = max(lastmod_dates).isoformat(timespec="seconds") + "Z"

    root = ET.ElementTree(sitemapindex)
    root.write(fname, encoding="utf-8", xml_declaration=True)
